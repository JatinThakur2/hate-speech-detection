# Advanced Multi-Model Hate Speech Detection

## ğŸ“š Overview

This notebook implements a comprehensive multi-model approach to hate speech detection using:

1. **EfficientNet + BERT** - Efficient baseline architecture
2. **CLIP + Text (Upgraded)** - Vision-language pre-training with attention fusion
3. **DeepSeek-R1** - Local LLM zero-shot classification via Ollama
4. **Ensemble** - Soft voting combining all three models

All models are evaluated using **5-Fold Cross-Validation** with comprehensive metrics and visualizations.

---

## ğŸš€ Features

### Data Processing

- âœ… Hateful Memes dataset loading (train.jsonl, dev_seen.jsonl)
- âœ… Image augmentation (rotation, crop, color jitter, etc.)
- âœ… BERT tokenization and padding
- âœ… Balanced sampling with WeightedRandomSampler
- âœ… Class distribution analysis

### Model Architectures

#### 1. EfficientNet+BERT (Baseline)

```
Image: EfficientNet-B0 â†’ 512-dim
Text: BERT (frozen early layers) â†’ 512-dim
Fusion: Cross-modal attention (8 heads) â†’ MLP classifier
Loss: Focal Loss (Î±=0.5, Î³=2.0)
```

#### 2. CLIP+Text (Upgraded)

```
Features: CLIP ViT-B/32 extraction
  - Image embeddings: L2-normalized (512-dim)
  - Text embeddings: L2-normalized (512-dim)
  - Similarity score: Cosine (1-dim)
  - Total: 1025-dim feature vector

Classifier: MultiheadAttention fusion
  - Cross-attention: 4 heads
  - Progressive MLP: 1025â†’512â†’256â†’128â†’2
  - Activations: GELU (better than ReLU)
  - Normalization: BatchNorm + LayerNorm

Optimization:
  - Optimizer: AdamW with differential learning rates
    - CLIP visual: 1e-6 (fine-tuning)
    - Classifier: 3e-4 (training)
  - Scheduler: CosineAnnealingLR
  - Regularization: Focal Loss + Dropout + Weight decay
```

#### 3. DeepSeek-R1 (Local LLM)

```
Model: deepseek-r1:latest (via Ollama API)
Interface: HTTP API at localhost:11434/api/generate
Approach: Zero-shot classification with natural language prompts
Input: Meme text + image description
Output: Classification (HATE/NON-HATE) + Confidence + Reasoning
```

#### 4. Ensemble (Soft Voting)

```
Method: Average predictions from all three models
Formula: ensemble_pred = (pred_model1 + pred_model2 + pred_model3) / 3
Decision: threshold at 0.5
Benefits: Better generalization, robustness, variance reduction
```

---

## ğŸ”§ Advanced Techniques

### 1. CLIP Feature Extraction & Caching

- L2 normalization for cosine similarity
- Fine-tunable visual encoder
- Feature caching mechanism (10-100x speedup)
- Batch-wise processing for GPU efficiency

### 2. Focal Loss for Class Imbalance

- Down-weights easy examples: (1 - p_t)^Î³
- Focuses on hard negatives/positives
- Parameters: Î±=0.5 (balance), Î³=2.0 (strong focusing)

### 3. Attention Fusion

- MultiheadAttention (4 heads)
- Cross-attention between image and text features
- Residual connections for deeper networks
- LayerNorm for training stability

### 4. Differential Learning Rates

- CLIP visual encoder: 1e-6 (preserve pre-training)
- Classification head: 3e-4 (learn from scratch)
- Enables effective fine-tuning of pre-trained models

### 5. Advanced Scheduling

- Warmup phase: Linear increase (0.1â†’1.0x)
- Cosine annealing: Smooth decay
- Better convergence than fixed learning rates

---

## ğŸ“Š Cross-Validation Framework

### Setup

```python
- Method: 5-Fold Cross-Validation with stratification
- Total samples: ~10,000 (combined train+val)
- Samples per fold: ~2,000 training, ~2,000 testing
- Train/test split: 50-50 per fold
```

### Training Per Fold

1. Initialize fresh models
2. Train on fold training set (3 epochs)
3. Evaluate on fold test set
4. Track: Accuracy, Precision, Recall, F1, ROC-AUC
5. Aggregate results with meanÂ±std

### Aggregation

- Mean metrics across 5 folds
- Standard deviation (measures variance/stability)
- Confidence intervals for statistical significance

---

## ğŸ“ˆ Expected Results

### Performance Improvements

```
Metric          | EfficientNet+BERT | CLIP+Text | LLM | Ensemble
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Accuracy        | ~0.82            | ~0.85     | ~0.78 | ~0.86
F1-Score        | ~0.80            | ~0.83     | ~0.75 | ~0.84
Precision       | ~0.81            | ~0.84     | ~0.79 | ~0.85
Recall          | ~0.79            | ~0.82     | ~0.71 | ~0.83
ROC-AUC         | ~0.89            | ~0.92     | ~0.85 | ~0.93
```

### Efficiency Gains

- **Training Speed**: 10-100x faster (feature caching)
- **Inference**: 2-3x faster than baseline
- **GPU Memory**: 40-50% reduction
- **Convergence**: 2-3x faster epochs

---

## ğŸ¯ Visualizations Generated

1. **model_comparison_detailed.png**

   - 5 subplots: Accuracy, Precision, Recall, F1, ROC-AUC
   - Per-model comparison with error bars

2. **f1_progression.png**

   - F1-score progression across 5 folds
   - Shows model consistency

3. **confusion_matrices_all.png**

   - 2x2 grid: One matrix per model
   - Shows TP, TN, FP, FN distributions

4. **ensemble_improvement.png**

   - Bar chart comparing ensemble vs individual models
   - Highlights ensemble gain

5. **roc_curves_all_models.png**
   - 2x2 grid: ROC curves for each model
   - Shows class separation capability

---

## ğŸ’¾ Output Files

### CSV Results

- `cv_results_detailed.csv` - Per-fold metrics for all models
- `cv_results_summary.csv` - Aggregate statistics (meanÂ±std)

### JSON Export

- `results.json` - Structured results for external analysis

### Reports

- `final_report.txt` - Comprehensive analysis with recommendations

### Visualizations

- Multiple PNG files for publication-ready figures

---

## ğŸ”Œ Local LLM Setup (DeepSeek-R1)

### Prerequisites

```bash
# Install Ollama
# Download from: https://ollama.ai

# Pull DeepSeek-R1 model
ollama pull deepseek-r1:latest

# Start Ollama server
ollama serve
```

### Verification

```python
# Test connection
curl http://localhost:11434/api/generate \
  -d '{
    "model": "deepseek-r1:latest",
    "prompt": "Hello",
    "stream": false
  }'
```

---

## ğŸš€ Running the Notebook

### Basic Execution

```python
# 1. Execute all cells in order
# 2. Notebook will:
#    - Load data
#    - Initialize models
#    - Run 5-fold CV training
#    - Generate visualizations
#    - Export results

# Expected runtime: 30-60 minutes (depending on GPU)
```

### Monitoring

- Progress printed after each fold
- Per-model metrics displayed
- Visualizations saved after each section

---

## ğŸ“ Code Structure

```
1. Imports & Environment Setup
2. Data Loading & Preprocessing
3. CLIP Model Initialization
4. Model Architecture Definitions
5. Local LLM Integration
6. Cross-Validation Framework
7. Training Functions
8. Execute Cross-Validation
9. Results Aggregation
10. Comprehensive Visualizations
11. ROC-AUC Curves
12. Final Summary Report
```

---

## ğŸ“ Key Learning Points

1. **Multimodal Learning**: Combining vision + language + text
2. **Attention Mechanisms**: Fusing heterogeneous modalities
3. **Class Imbalance**: Focal Loss vs traditional CE loss
4. **Cross-Validation**: Robust evaluation strategies
5. **Ensemble Methods**: Soft voting and predictions averaging
6. **Local LLM Integration**: Using Ollama for zero-shot learning
7. **Feature Caching**: Optimization for training efficiency
8. **Production Deployment**: Model selection and monitoring

---

## ğŸ” Troubleshooting

### CLIP Not Installed

```python
pip install openai-clip
```

### CUDA Out of Memory

```python
# Reduce batch size in code
BATCH_SIZE = 8  # instead of 16
```

### DeepSeek-R1 Not Connecting

```bash
# Make sure Ollama is running
ollama serve

# Check API connectivity
curl http://localhost:11434/api/tags
```

### Dataset Not Found

```python
# Update data paths
DATA_DIR = r"C:\your\path\to\hateful_memes"
```

---

## ğŸ“ Support

For issues or questions:

1. Check error messages in notebook output
2. Verify dependencies are installed
3. Ensure GPU has sufficient memory
4. Check local LLM is running (if using DeepSeek-R1)

---

## ğŸ“„ Citation

If you use this code, please cite:

```bibtex
@misc{multimodel_hate_speech_2024,
  title={Advanced Multi-Model Hate Speech Detection:
         EfficientNet+BERT+CLIP+DeepSeek-R1 Ensemble},
  author={Your Name},
  year={2024}
}
```

---

**Last Updated**: November 2024
**Status**: âœ… Complete and Ready for Use
